Implementing Transformer Architecture as given in the brilliant paper "Attention Is All You Need"
#### Ideas implemented in this code: 
- Self Attention
- Multihead Attention
- Positional Encoding
- Cross Attention
- Masking
- Transforemer
